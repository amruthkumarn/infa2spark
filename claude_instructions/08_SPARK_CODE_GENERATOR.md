# 08_SPARK_CODE_GENERATOR.md

## ‚ö° PySpark Code Generation Engine Implementation

Implement the comprehensive PySpark code generator that converts XSD-compliant Informatica objects into production-ready Spark applications with professional formatting and enterprise features.

## üìã Prerequisites

- **XSD Base Architecture** completed (from `04_XSD_BASE_ARCHITECTURE.md`)
- **XML Parser Implementation** completed (from `05_XML_PARSER_IMPLEMENTATION.md`)
- **Configuration System** implemented (from `03_CONFIGURATION_FILES.md`)
- **Informatica XSD schemas** available in `/informatica_xsd_xml/` directory

## üéØ Code Generator Architecture Overview

The Spark code generator provides:
- **Professional PySpark code generation** with Black formatting
- **Complete application structure** with deployment files
- **Configuration externalization** with YAML-based settings
- **Multiple template systems** (lean, ultra-lean, enterprise)
- **DAG optimization** with parallel execution planning
- **Enterprise monitoring** integration
- **Docker containerization** support

## üìÅ Implementation Structure

```
src/core/
‚îú‚îÄ‚îÄ spark_code_generator.py              # Main code generator
‚îú‚îÄ‚îÄ enhanced_spark_generator.py          # Enhanced generation features
‚îú‚îÄ‚îÄ mapping_dag_processor.py             # DAG analysis and optimization
‚îú‚îÄ‚îÄ workflow_dag_processor.py            # Workflow orchestration
‚îú‚îÄ‚îÄ config_externalization.py            # Configuration management
‚îú‚îÄ‚îÄ config_file_generator.py             # Config file generation
‚îî‚îÄ‚îÄ templates/                           # Code templates
    ‚îú‚îÄ‚îÄ lean_mapping_template.py
    ‚îú‚îÄ‚îÄ ultra_lean_mapping_template.py
    ‚îî‚îÄ‚îÄ enterprise_ultra_lean_template.py
```

## üîß Core Implementation

### 1. src/core/spark_code_generator.py

```python
"""
Spark Code Generator for Informatica BDM Projects
Generates standalone PySpark applications from Informatica XML projects
"""
import os
import json
from typing import Dict, List, Any, Optional
from pathlib import Path
import xml.etree.ElementTree as ET
from jinja2 import Template, Environment, FileSystemLoader
import logging
import yaml
from datetime import datetime
import subprocess

from .xml_parser import InformaticaXMLParser
from .workflow_dag_processor import WorkflowDAGProcessor
from .mapping_dag_processor import MappingDAGProcessor
from .base_classes import Project
from .enhanced_spark_generator import EnhancedSparkCodeGenerator
from .config_externalization import MappingConfigurationManager, RuntimeConfigResolver
from .config_file_generator import ConfigurationFileGenerator

class SparkCodeGenerator:
    """
    Generates complete PySpark applications from Informatica XML projects
    
    Creates a standalone project structure with:
    - Generated mapping classes with professional formatting
    - Workflow orchestration with DAG optimization
    - Configuration files with externalization
    - Deployment scripts (Docker, run.sh)
    - Comprehensive documentation
    - Test data generation capabilities
    """
    
    def __init__(self, output_base_dir: str = "generated_spark_apps", 
                 enable_config_externalization: bool = True, 
                 enterprise_features: bool = True):
        self.output_base_dir = Path(output_base_dir)
        self.logger = logging.getLogger("SparkCodeGenerator")
        self.xml_parser = InformaticaXMLParser()
        self.enhanced_generator = EnhancedSparkCodeGenerator(output_base_dir)
        self.enable_config_externalization = enable_config_externalization
        self.enterprise_features = enterprise_features
        
        # Template environment for code generation
        self.jinja_env = Environment(
            loader=FileSystemLoader(Path(__file__).parent / "templates"),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # Generation statistics
        self.generation_stats = {
            'start_time': None,
            'end_time': None,
            'total_mappings': 0,
            'total_workflows': 0,
            'total_sessions': 0,
            'total_transformations': 0,
            'generated_files': 0,
            'errors': 0
        }
        
    def generate_spark_application(self, xml_file_path: str, app_name: str, 
                                 output_dir: str = None) -> str:
        """
        Generate a complete Spark application from Informatica XML
        
        Args:
            xml_file_path: Path to Informatica XML project file
            app_name: Name for the generated application
            output_dir: Optional custom output directory
            
        Returns:
            Path to generated application directory
        """
        self.generation_stats['start_time'] = datetime.now()
        
        try:
            self.logger.info(f"Starting Spark application generation")
            self.logger.info(f"XML file: {xml_file_path}")
            self.logger.info(f"Application name: {app_name}")
            
            # Parse Informatica XML project
            project = self._parse_informatica_project(xml_file_path)
            if not project:
                raise ValueError(f"Failed to parse XML project: {xml_file_path}")
                
            # Set up output directory
            if output_dir:
                app_output_dir = Path(output_dir) / app_name
            else:
                app_output_dir = self.output_base_dir / app_name
                
            # Create application structure
            self._create_application_structure(app_output_dir)
            
            # Generate core application files
            self._generate_application_files(project, app_output_dir, app_name)
            
            # Generate configuration files
            if self.enable_config_externalization:
                self._generate_configuration_files(project, app_output_dir)
                
            # Generate deployment files
            self._generate_deployment_files(app_output_dir, app_name)
            
            # Generate documentation
            self._generate_documentation(project, app_output_dir, app_name)
            
            # Apply professional formatting
            self._apply_code_formatting(app_output_dir)
            
            self.generation_stats['end_time'] = datetime.now()
            duration = (self.generation_stats['end_time'] - self.generation_stats['start_time']).total_seconds()
            
            self.logger.info(f"‚úÖ Spark application generated successfully in {duration:.2f}s")
            self.logger.info(f"üìÅ Application location: {app_output_dir}")
            self.logger.info(f"üìä Generation statistics: {self._get_generation_summary()}")
            
            return str(app_output_dir)
            
        except Exception as e:
            self.generation_stats['errors'] += 1
            self.logger.error(f"‚ùå Application generation failed: {str(e)}")
            raise
            
    def _parse_informatica_project(self, xml_file_path: str) -> Optional[Project]:
        """Parse Informatica XML project"""
        try:
            self.logger.info(f"üîç Parsing Informatica project: {xml_file_path}")
            
            # Use the XML parser to load the project
            project = self.xml_parser.parse_project(xml_file_path)
            
            if project:
                self.logger.info(f"‚úÖ Successfully parsed project: {project.name}")
                self.logger.info(f"üìã Project version: {project.version}")
                self.logger.info(f"üìÅ Folders found: {len(project.folders)}")
                
                # Update statistics
                self.generation_stats['total_mappings'] = sum(
                    len(folder.mappings) for folder in project.folders
                )
                self.generation_stats['total_workflows'] = sum(
                    len(folder.workflows) for folder in project.folders
                )
                self.generation_stats['total_sessions'] = sum(
                    len(folder.sessions) for folder in project.folders
                )
                
            return project
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to parse project: {str(e)}")
            return None
            
    def _create_application_structure(self, app_dir: Path):
        """Create the complete application directory structure"""
        self.logger.info(f"üèóÔ∏è  Creating application structure: {app_dir}")
        
        # Main directories
        directories = [
            app_dir,
            app_dir / "src" / "main" / "python",
            app_dir / "src" / "main" / "python" / "mappings",
            app_dir / "src" / "main" / "python" / "transformations", 
            app_dir / "src" / "main" / "python" / "workflows",
            app_dir / "config",
            app_dir / "config" / "component-metadata",
            app_dir / "config" / "dag-analysis",
            app_dir / "config" / "execution-plans",
            app_dir / "config" / "runtime",
            app_dir / "data" / "input",
            app_dir / "data" / "output",
            app_dir / "logs",
            app_dir / "scripts"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            self.logger.debug(f"üìÅ Created directory: {directory}")
            
        # Create __init__.py files
        init_files = [
            app_dir / "src" / "main" / "python" / "__init__.py",
            app_dir / "src" / "main" / "python" / "mappings" / "__init__.py",
            app_dir / "src" / "main" / "python" / "transformations" / "__init__.py",
            app_dir / "src" / "main" / "python" / "workflows" / "__init__.py"
        ]
        
        for init_file in init_files:
            init_file.write_text("")
            
    def _generate_application_files(self, project: Project, app_dir: Path, app_name: str):
        """Generate core application files"""
        self.logger.info(f"‚ö° Generating core application files")
        
        # Generate main.py
        self._generate_main_file(project, app_dir, app_name)
        
        # Generate base classes
        self._generate_base_classes(app_dir)
        
        # Generate mappings
        self._generate_mappings(project, app_dir)
        
        # Generate workflows
        self._generate_workflows(project, app_dir)
        
        # Generate transformations
        self._generate_transformations(project, app_dir)
        
        # Generate enterprise components if enabled
        if self.enterprise_features:
            self._generate_enterprise_components(app_dir)
            
    def _generate_main_file(self, project: Project, app_dir: Path, app_name: str):
        """Generate the main application entry point"""
        main_template = self.jinja_env.get_template("main_app_template.py")
        
        main_content = main_template.render(
            app_name=app_name,
            project_name=project.name,
            project_version=project.version,
            generation_timestamp=datetime.now().isoformat(),
            mappings=[m.name for folder in project.folders for m in folder.mappings],
            workflows=[w.name for folder in project.folders for w in folder.workflows]
        )
        
        main_file = app_dir / "src" / "main" / "python" / "main.py"
        main_file.write_text(main_content)
        self.generation_stats['generated_files'] += 1
        
        self.logger.debug(f"üìÑ Generated main.py")
        
    def _generate_base_classes(self, app_dir: Path):
        """Generate base classes for the application"""
        base_classes_template = self.jinja_env.get_template("base_classes_template.py")
        
        base_classes_content = base_classes_template.render(
            generation_timestamp=datetime.now().isoformat()
        )
        
        base_classes_file = app_dir / "src" / "main" / "python" / "base_classes.py"
        base_classes_file.write_text(base_classes_content)
        self.generation_stats['generated_files'] += 1
        
    def _generate_mappings(self, project: Project, app_dir: Path):
        """Generate mapping classes"""
        self.logger.info(f"üó∫Ô∏è  Generating mappings")
        
        for folder in project.folders:
            for mapping in folder.mappings:
                self._generate_single_mapping(mapping, app_dir, project)
                
    def _generate_single_mapping(self, mapping, app_dir: Path, project: Project):
        """Generate a single mapping class"""
        try:
            # Analyze mapping DAG for optimization
            dag_processor = MappingDAGProcessor()
            dag_analysis = dag_processor.process_mapping_dag(mapping)
            
            # Select appropriate template based on complexity
            template_name = self._select_mapping_template(mapping, dag_analysis)
            mapping_template = self.jinja_env.get_template(template_name)
            
            # Prepare template context
            template_context = {
                'mapping': mapping,
                'project': project,
                'dag_analysis': dag_analysis,
                'generation_timestamp': datetime.now().isoformat(),
                'transformations': self._get_mapping_transformations(mapping),
                'instances': self._get_mapping_instances(mapping),
                'connections': self._get_mapping_connections(mapping),
                'enterprise_features': self.enterprise_features
            }
            
            # Generate mapping code
            mapping_content = mapping_template.render(**template_context)
            
            # Write mapping file
            mapping_file_name = f"{mapping.name.lower().replace(' ', '_')}.py"
            mapping_file = app_dir / "src" / "main" / "python" / "mappings" / mapping_file_name
            mapping_file.write_text(mapping_content)
            
            self.generation_stats['generated_files'] += 1
            self.logger.debug(f"üìÑ Generated mapping: {mapping.name}")
            
            # Generate DAG analysis file
            if dag_analysis:
                self._generate_dag_analysis_file(mapping, dag_analysis, app_dir)
                
            # Generate component metadata
            self._generate_component_metadata(mapping, app_dir)
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to generate mapping {mapping.name}: {str(e)}")
            self.generation_stats['errors'] += 1
            
    def _select_mapping_template(self, mapping, dag_analysis) -> str:
        """Select appropriate template based on mapping complexity"""
        transformation_count = len(getattr(mapping, 'transformations', []))
        
        if self.enterprise_features and transformation_count > 10:
            return "enterprise_ultra_lean_template.py"
        elif transformation_count > 5:
            return "ultra_lean_mapping_template.py"
        else:
            return "lean_mapping_template.py"
            
    def _generate_workflows(self, project: Project, app_dir: Path):
        """Generate workflow orchestration classes"""
        self.logger.info(f"üîÑ Generating workflows")
        
        for folder in project.folders:
            for workflow in folder.workflows:
                self._generate_single_workflow(workflow, app_dir, project)
                
    def _generate_single_workflow(self, workflow, app_dir: Path, project: Project):
        """Generate a single workflow class"""
        try:
            # Analyze workflow DAG
            workflow_processor = WorkflowDAGProcessor()
            workflow_dag = workflow_processor.process_workflow_dag(workflow)
            
            # Generate workflow code
            workflow_template = self.jinja_env.get_template("workflow_template.py")
            
            workflow_content = workflow_template.render(
                workflow=workflow,
                project=project,
                workflow_dag=workflow_dag,
                generation_timestamp=datetime.now().isoformat(),
                tasks=self._get_workflow_tasks(workflow),
                dependencies=workflow_dag.get('dependencies', []) if workflow_dag else []
            )
            
            # Write workflow file
            workflow_file_name = f"{workflow.name.lower().replace(' ', '_')}.py"
            workflow_file = app_dir / "src" / "main" / "python" / "workflows" / workflow_file_name
            workflow_file.write_text(workflow_content)
            
            self.generation_stats['generated_files'] += 1
            self.logger.debug(f"üìÑ Generated workflow: {workflow.name}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to generate workflow {workflow.name}: {str(e)}")
            self.generation_stats['errors'] += 1
            
    def _generate_transformations(self, project: Project, app_dir: Path):
        """Generate transformation implementation classes"""
        self.logger.info(f"üîß Generating transformations")
        
        # Generate generic transformations file
        transformations_template = self.jinja_env.get_template("transformations_template.py")
        
        # Collect all unique transformation types
        transformation_types = set()
        for folder in project.folders:
            for mapping in folder.mappings:
                for transformation in getattr(mapping, 'transformations', []):
                    transformation_types.add(getattr(transformation, 'type', 'Generic'))
                    
        transformations_content = transformations_template.render(
            transformation_types=list(transformation_types),
            generation_timestamp=datetime.now().isoformat(),
            enterprise_features=self.enterprise_features
        )
        
        transformations_file = app_dir / "src" / "main" / "python" / "transformations" / "generated_transformations.py"
        transformations_file.write_text(transformations_content)
        self.generation_stats['generated_files'] += 1
        
    def _generate_enterprise_components(self, app_dir: Path):
        """Generate enterprise-specific components"""
        self.logger.info(f"üè¢ Generating enterprise components")
        
        enterprise_components = [
            ("config_management.py", "config_management_template.py"),
            ("monitoring_integration.py", "monitoring_template.py"),
            ("advanced_config_validation.py", "validation_template.py"),
            ("config_migration_tools.py", "migration_template.py")
        ]
        
        for component_file, template_file in enterprise_components:
            try:
                template = self.jinja_env.get_template(template_file)
                content = template.render(
                    generation_timestamp=datetime.now().isoformat()
                )
                
                output_file = app_dir / "src" / "main" / "python" / component_file
                output_file.write_text(content)
                self.generation_stats['generated_files'] += 1
                
                self.logger.debug(f"üìÑ Generated enterprise component: {component_file}")
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è  Failed to generate {component_file}: {str(e)}")
                
    def _generate_configuration_files(self, project: Project, app_dir: Path):
        """Generate configuration files with externalization"""
        self.logger.info(f"‚öôÔ∏è  Generating configuration files")
        
        config_generator = ConfigurationFileGenerator()
        
        # Generate main application.yaml
        app_config = {
            'application': {
                'name': project.name,
                'version': project.version,
                'description': getattr(project, 'description', ''),
                'generated_on': datetime.now().isoformat()
            },
            'spark': self._get_spark_configuration(),
            'connections': self._get_connections_configuration(project),
            'parameters': self._get_project_parameters(project)
        }
        
        app_config_file = app_dir / "config" / "application.yaml"
        with open(app_config_file, 'w') as f:
            yaml.dump(app_config, f, default_flow_style=False, indent=2)
            
        self.generation_stats['generated_files'] += 1
        
        # Generate runtime configuration
        runtime_config = {
            'memory_profiles': self._generate_memory_profiles(project),
            'optimization_settings': self._generate_optimization_settings(),
            'monitoring_config': self._generate_monitoring_config()
        }
        
        runtime_config_file = app_dir / "config" / "runtime" / "memory-profiles.yaml"
        with open(runtime_config_file, 'w') as f:
            yaml.dump(runtime_config, f, default_flow_style=False, indent=2)
            
    def _generate_deployment_files(self, app_dir: Path, app_name: str):
        """Generate deployment files"""
        self.logger.info(f"üö¢ Generating deployment files")
        
        # Generate requirements.txt
        requirements_content = self._get_requirements_content()
        requirements_file = app_dir / "requirements.txt"
        requirements_file.write_text(requirements_content)
        
        # Generate run.sh
        run_script_content = self._get_run_script_content(app_name)
        run_script_file = app_dir / "run.sh"
        run_script_file.write_text(run_script_content)
        run_script_file.chmod(0o755)  # Make executable
        
        # Generate Dockerfile
        dockerfile_content = self._get_dockerfile_content(app_name)
        dockerfile = app_dir / "Dockerfile"
        dockerfile.write_text(dockerfile_content)
        
        # Generate docker-compose.yml
        docker_compose_content = self._get_docker_compose_content(app_name)
        docker_compose_file = app_dir / "docker-compose.yml"
        docker_compose_file.write_text(docker_compose_content)
        
        self.generation_stats['generated_files'] += 4
        
    def _generate_documentation(self, project: Project, app_dir: Path, app_name: str):
        """Generate comprehensive documentation"""
        self.logger.info(f"üìö Generating documentation")
        
        # Generate README.md
        readme_template = self.jinja_env.get_template("readme_template.md")
        readme_content = readme_template.render(
            app_name=app_name,
            project=project,
            generation_stats=self.generation_stats,
            generation_timestamp=datetime.now().isoformat()
        )
        
        readme_file = app_dir / "README.md"
        readme_file.write_text(readme_content)
        self.generation_stats['generated_files'] += 1
        
    def _apply_code_formatting(self, app_dir: Path):
        """Apply professional code formatting using Black"""
        self.logger.info(f"üé® Applying code formatting")
        
        try:
            # Format Python files with Black
            python_files = list((app_dir / "src").rglob("*.py"))
            
            for py_file in python_files:
                try:
                    result = subprocess.run(
                        ["black", "--line-length", "88", str(py_file)],
                        capture_output=True,
                        text=True,
                        timeout=30
                    )
                    
                    if result.returncode == 0:
                        self.logger.debug(f"‚úÖ Formatted: {py_file.name}")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è  Black formatting failed for {py_file.name}: {result.stderr}")
                        
                except subprocess.TimeoutExpired:
                    self.logger.warning(f"‚ö†Ô∏è  Black formatting timeout for {py_file.name}")
                except FileNotFoundError:
                    self.logger.warning("‚ö†Ô∏è  Black not found - skipping code formatting")
                    break
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  Code formatting failed: {str(e)}")
            
    # Helper methods for generating various components
    def _get_mapping_transformations(self, mapping):
        """Get transformations for a mapping"""
        return getattr(mapping, 'transformations', [])
        
    def _get_mapping_instances(self, mapping):
        """Get instances for a mapping"""
        return getattr(mapping, 'instances', [])
        
    def _get_mapping_connections(self, mapping):
        """Get connections used by a mapping"""
        return getattr(mapping, 'connections', [])
        
    def _get_workflow_tasks(self, workflow):
        """Get tasks for a workflow"""
        return getattr(workflow, 'tasks', [])
        
    def _get_spark_configuration(self) -> Dict[str, Any]:
        """Get Spark configuration for generated application"""
        return {
            "spark.app.name": "Generated_Spark_Application",
            "spark.master": "local[*]",
            "spark.driver.memory": "2g",
            "spark.executor.memory": "2g",
            "spark.sql.adaptive.enabled": "true",
            "spark.sql.adaptive.coalescePartitions.enabled": "true"
        }
        
    def _get_connections_configuration(self, project: Project) -> Dict[str, Any]:
        """Get connections configuration"""
        # Extract connections from project
        connections = {}
        for folder in project.folders:
            for connection in getattr(folder, 'connections', []):
                connections[connection.name] = {
                    'type': getattr(connection, 'type', 'UNKNOWN'),
                    'host': getattr(connection, 'host', 'localhost'),
                    'port': getattr(connection, 'port', 0),
                    'database': getattr(connection, 'database', ''),
                    'local_path': 'sample_data/'  # For development
                }
        return connections
        
    def _get_project_parameters(self, project: Project) -> Dict[str, Any]:
        """Get project parameters"""
        return getattr(project, 'parameters', {})
        
    def _generate_memory_profiles(self, project: Project) -> Dict[str, Any]:
        """Generate memory profiles for different components"""
        return {
            'small_mapping': {'driver_memory': '1g', 'executor_memory': '2g'},
            'medium_mapping': {'driver_memory': '2g', 'executor_memory': '4g'},
            'large_mapping': {'driver_memory': '4g', 'executor_memory': '8g'}
        }
        
    def _generate_optimization_settings(self) -> Dict[str, Any]:
        """Generate optimization settings"""
        return {
            'adaptive_query_execution': True,
            'coalesce_partitions': True,
            'broadcast_threshold': '10MB',
            'max_broadcast_table_bytes': '8GB'
        }
        
    def _generate_monitoring_config(self) -> Dict[str, Any]:
        """Generate monitoring configuration"""
        return {
            'metrics_enabled': True,
            'log_level': 'INFO',
            'performance_monitoring': True
        }
        
    def _generate_dag_analysis_file(self, mapping, dag_analysis, app_dir: Path):
        """Generate DAG analysis file"""
        dag_file = app_dir / "config" / "dag-analysis" / f"{mapping.name}_dag_analysis.json"
        with open(dag_file, 'w') as f:
            json.dump(dag_analysis, f, indent=2, default=str)
            
    def _generate_component_metadata(self, mapping, app_dir: Path):
        """Generate component metadata file"""
        metadata = {
            'mapping_name': mapping.name,
            'components': len(getattr(mapping, 'transformations', [])),
            'generated_on': datetime.now().isoformat()
        }
        
        metadata_file = app_dir / "config" / "component-metadata" / f"{mapping.name}_components.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
            
    def _get_requirements_content(self) -> str:
        """Generate requirements.txt content"""
        return """# Generated Spark Application Requirements
pyspark>=3.4.0
py4j>=0.10.9.7
PyYAML>=6.0
pandas>=1.5.0
"""
        
    def _get_run_script_content(self, app_name: str) -> str:
        """Generate run.sh script content"""
        return f"""#!/bin/bash
# Generated run script for {app_name}

set -e

echo "üöÄ Starting {app_name}"

# Check if Spark is available
if ! command -v spark-submit &> /dev/null; then
    echo "‚ùå spark-submit not found. Please install Apache Spark."
    exit 1
fi

# Run the Spark application
spark-submit \\
    --master local[*] \\
    --conf spark.sql.adaptive.enabled=true \\
    --conf spark.sql.adaptive.coalescePartitions.enabled=true \\
    src/main/python/main.py \\
    --config config/application.yaml

echo "‚úÖ {app_name} completed"
"""
        
    def _get_dockerfile_content(self, app_name: str) -> str:
        """Generate Dockerfile content"""
        return f"""# Generated Dockerfile for {app_name}
FROM python:3.11-slim

# Install Java (required for PySpark)
RUN apt-get update && apt-get install -y openjdk-11-jre-headless && rm -rf /var/lib/apt/lists/*
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application files
COPY . .

# Make run script executable
RUN chmod +x run.sh

# Default command
CMD ["./run.sh"]
"""
        
    def _get_docker_compose_content(self, app_name: str) -> str:
        """Generate docker-compose.yml content"""
        return f"""# Generated docker-compose.yml for {app_name}
version: '3.8'

services:
  {app_name.lower().replace('_', '-')}:
    build: .
    container_name: {app_name.lower()}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    environment:
      - SPARK_MASTER=local[*]
    ports:
      - "4040:4040"  # Spark UI
"""
        
    def _get_generation_summary(self) -> Dict[str, Any]:
        """Get generation summary statistics"""
        return {
            'total_files': self.generation_stats['generated_files'],
            'mappings': self.generation_stats['total_mappings'],
            'workflows': self.generation_stats['total_workflows'],
            'errors': self.generation_stats['errors'],
            'duration': (
                (self.generation_stats['end_time'] - self.generation_stats['start_time']).total_seconds()
                if self.generation_stats['end_time'] and self.generation_stats['start_time']
                else 0
            )
        }
```

### 2. src/core/mapping_dag_processor.py

```python
"""
Mapping DAG Processor for analyzing and optimizing transformation dependencies
"""
import logging
from typing import Dict, List, Any, Optional, Set, Tuple
from collections import defaultdict, deque
import json

class MappingDAGProcessor:
    """
    Processes mapping transformations to create optimized execution DAGs
    
    Features:
    - Dependency analysis and topological sorting
    - Parallel execution group identification
    - Resource requirement estimation
    - Execution time prediction
    - DAG validation and cycle detection
    """
    
    def __init__(self):
        self.logger = logging.getLogger("MappingDAGProcessor")
        
    def process_mapping_dag(self, mapping) -> Dict[str, Any]:
        """
        Process a mapping to create optimized execution DAG
        
        Args:
            mapping: Mapping object to process
            
        Returns:
            DAG analysis with execution plan
        """
        try:
            self.logger.info(f"Processing mapping DAG: {mapping.name}")
            
            # Extract transformations and dependencies
            transformations = self._extract_transformations(mapping)
            dependencies = self._analyze_dependencies(transformations)
            
            # Perform topological sort
            execution_order = self._topological_sort(transformations, dependencies)
            
            # Identify parallel execution groups
            parallel_groups = self._identify_parallel_groups(transformations, dependencies)
            
            # Estimate resource requirements
            resource_estimates = self._estimate_resources(transformations)
            
            # Generate execution plan
            execution_plan = self._generate_execution_plan(
                transformations, execution_order, parallel_groups, resource_estimates
            )
            
            dag_analysis = {
                'mapping_name': mapping.name,
                'transformation_count': len(transformations),
                'dependency_count': len(dependencies),
                'parallel_groups': len(parallel_groups),
                'execution_levels': len(execution_order),
                'estimated_duration_seconds': self._estimate_total_duration(transformations),
                'transformations': transformations,
                'dependencies': dependencies,
                'execution_order': execution_order,
                'parallel_groups': parallel_groups,
                'resource_estimates': resource_estimates,
                'execution_plan': execution_plan,
                'optimization_opportunities': self._identify_optimizations(transformations, dependencies)
            }
            
            self.logger.info(f"‚úÖ DAG analysis complete: {len(transformations)} components, "
                           f"{len(parallel_groups)} parallel groups, "
                           f"{len(execution_order)} execution levels")
            
            return dag_analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to process mapping DAG: {str(e)}")
            return {}
            
    def _extract_transformations(self, mapping) -> Dict[str, Dict[str, Any]]:
        """Extract transformation information from mapping"""
        transformations = {}
        
        # Get transformations from mapping
        for transformation in getattr(mapping, 'transformations', []):
            trans_info = {
                'name': getattr(transformation, 'name', 'Unknown'),
                'type': getattr(transformation, 'type', 'Generic'),
                'active': getattr(transformation, 'active', True),
                'complexity': self._estimate_complexity(transformation),
                'estimated_duration': self._estimate_duration(transformation),
                'memory_requirement': self._estimate_memory(transformation),
                'cpu_requirement': self._estimate_cpu(transformation),
                'input_ports': getattr(transformation, 'input_ports', []),
                'output_ports': getattr(transformation, 'output_ports', [])
            }
            transformations[trans_info['name']] = trans_info
            
        return transformations
        
    def _analyze_dependencies(self, transformations: Dict[str, Dict[str, Any]]) -> List[Tuple[str, str]]:
        """Analyze dependencies between transformations"""
        dependencies = []
        
        # Analyze port connections to determine dependencies
        for trans_name, trans_info in transformations.items():
            for input_port in trans_info.get('input_ports', []):
                source_trans = getattr(input_port, 'source_transformation', None)
                if source_trans and source_trans in transformations:
                    dependencies.append((source_trans, trans_name))
                    
        return dependencies
        
    def _topological_sort(self, transformations: Dict[str, Dict[str, Any]], 
                         dependencies: List[Tuple[str, str]]) -> List[List[str]]:
        """Perform topological sort to determine execution order"""
        # Build adjacency list and in-degree count
        graph = defaultdict(list)
        in_degree = defaultdict(int)
        
        # Initialize all transformations with in-degree 0
        for trans_name in transformations:
            in_degree[trans_name] = 0
            
        # Build graph and calculate in-degrees
        for source, target in dependencies:
            graph[source].append(target)
            in_degree[target] += 1
            
        # Perform level-wise topological sort
        execution_levels = []
        queue = deque([trans for trans in transformations if in_degree[trans] == 0])
        
        while queue:
            current_level = []
            level_size = len(queue)
            
            for _ in range(level_size):
                trans = queue.popleft()
                current_level.append(trans)
                
                # Reduce in-degree of neighbors
                for neighbor in graph[trans]:
                    in_degree[neighbor] -= 1
                    if in_degree[neighbor] == 0:
                        queue.append(neighbor)
                        
            if current_level:
                execution_levels.append(current_level)
                
        return execution_levels
        
    def _identify_parallel_groups(self, transformations: Dict[str, Dict[str, Any]], 
                                dependencies: List[Tuple[str, str]]) -> List[List[str]]:
        """Identify transformations that can be executed in parallel"""
        # Transformations in the same execution level can potentially run in parallel
        execution_order = self._topological_sort(transformations, dependencies)
        
        parallel_groups = []
        for level in execution_order:
            if len(level) > 1:
                # Further analyze if transformations in this level can truly run in parallel
                # (e.g., no resource conflicts, no data dependencies within the level)
                parallel_groups.append(level)
                
        return parallel_groups
        
    def _estimate_resources(self, transformations: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """Estimate resource requirements for each transformation"""
        resource_estimates = {}
        
        for trans_name, trans_info in transformations.items():
            resource_estimates[trans_name] = {
                'memory_mb': trans_info['memory_requirement'],
                'cpu_cores': trans_info['cpu_requirement'],
                'io_intensive': self._is_io_intensive(trans_info['type']),
                'network_intensive': self._is_network_intensive(trans_info['type']),
                'estimated_duration_seconds': trans_info['estimated_duration']
            }
            
        return resource_estimates
        
    def _generate_execution_plan(self, transformations: Dict[str, Dict[str, Any]], 
                               execution_order: List[List[str]], 
                               parallel_groups: List[List[str]], 
                               resource_estimates: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Generate optimized execution plan"""
        phases = []
        total_estimated_time = 0
        
        for i, level in enumerate(execution_order):
            phase = {
                'phase_number': i + 1,
                'transformations': level,
                'can_parallelize': len(level) > 1,
                'estimated_duration': max(
                    resource_estimates[trans]['estimated_duration_seconds'] 
                    for trans in level
                ),
                'total_memory_required': sum(
                    resource_estimates[trans]['memory_mb'] 
                    for trans in level
                ),
                'total_cpu_cores': sum(
                    resource_estimates[trans]['cpu_cores'] 
                    for trans in level
                )
            }
            phases.append(phase)
            total_estimated_time += phase['estimated_duration']
            
        return {
            'phases': phases,
            'total_phases': len(phases),
            'total_estimated_duration': total_estimated_time,
            'parallelizable_phases': len([p for p in phases if p['can_parallelize']]),
            'optimization_applied': True
        }
        
    def _estimate_complexity(self, transformation) -> str:
        """Estimate transformation complexity"""
        trans_type = getattr(transformation, 'type', 'Generic')
        
        complexity_map = {
            'Source': 'LOW',
            'Target': 'LOW', 
            'Expression': 'MEDIUM',
            'Aggregator': 'HIGH',
            'Lookup': 'HIGH',
            'Joiner': 'HIGH',
            'Sorter': 'MEDIUM',
            'Router': 'MEDIUM',
            'Union': 'LOW',
            'Sequence': 'LOW'
        }
        
        return complexity_map.get(trans_type, 'MEDIUM')
        
    def _estimate_duration(self, transformation) -> int:
        """Estimate transformation execution duration in seconds"""
        trans_type = getattr(transformation, 'type', 'Generic')
        
        duration_map = {
            'Source': 5,
            'Target': 10,
            'Expression': 3,
            'Aggregator': 15,
            'Lookup': 12,
            'Joiner': 18,
            'Sorter': 8,
            'Router': 5,
            'Union': 3,
            'Sequence': 2
        }
        
        return duration_map.get(trans_type, 5)
        
    def _estimate_memory(self, transformation) -> int:
        """Estimate memory requirement in MB"""
        trans_type = getattr(transformation, 'type', 'Generic')
        
        memory_map = {
            'Source': 512,
            'Target': 512,
            'Expression': 256,
            'Aggregator': 2048,
            'Lookup': 1024,
            'Joiner': 2048,
            'Sorter': 1024,
            'Router': 512,
            'Union': 256,
            'Sequence': 128
        }
        
        return memory_map.get(trans_type, 512)
        
    def _estimate_cpu(self, transformation) -> float:
        """Estimate CPU requirement in cores"""
        trans_type = getattr(transformation, 'type', 'Generic')
        
        cpu_map = {
            'Source': 0.5,
            'Target': 0.5,
            'Expression': 1.0,
            'Aggregator': 2.0,
            'Lookup': 1.5,
            'Joiner': 2.0,
            'Sorter': 1.5,
            'Router': 1.0,
            'Union': 0.5,
            'Sequence': 0.25
        }
        
        return cpu_map.get(trans_type, 1.0)
        
    def _is_io_intensive(self, trans_type: str) -> bool:
        """Check if transformation is I/O intensive"""
        io_intensive_types = {'Source', 'Target', 'Lookup'}
        return trans_type in io_intensive_types
        
    def _is_network_intensive(self, trans_type: str) -> bool:
        """Check if transformation is network intensive"""
        network_intensive_types = {'Lookup', 'Joiner'}
        return trans_type in network_intensive_types
        
    def _estimate_total_duration(self, transformations: Dict[str, Dict[str, Any]]) -> int:
        """Estimate total execution duration"""
        return sum(trans['estimated_duration'] for trans in transformations.values())
        
    def _identify_optimizations(self, transformations: Dict[str, Dict[str, Any]], 
                              dependencies: List[Tuple[str, str]]) -> List[str]:
        """Identify optimization opportunities"""
        optimizations = []
        
        # Check for pushdown opportunities
        source_count = sum(1 for trans in transformations.values() if trans['type'] == 'Source')
        if source_count > 1:
            optimizations.append("Consider source consolidation for better performance")
            
        # Check for caching opportunities
        lookup_count = sum(1 for trans in transformations.values() if trans['type'] == 'Lookup')
        if lookup_count > 2:
            optimizations.append("Multiple lookups detected - consider caching lookup tables")
            
        # Check for join optimization
        joiner_count = sum(1 for trans in transformations.values() if trans['type'] == 'Joiner')
        if joiner_count > 1:
            optimizations.append("Multiple joins detected - consider join order optimization")
            
        return optimizations
```

## ‚úÖ Template System

Create the template files referenced in the code generator:

### src/core/templates/enterprise_ultra_lean_template.py

```python
"""
Enterprise Ultra-Lean Mapping Template
Professional PySpark mapping with enterprise features
"""

ENTERPRISE_ULTRA_LEAN_TEMPLATE = '''
"""
{{ mapping.name }} - Generated Mapping Class
Auto-generated from Informatica BDM on {{ generation_timestamp }}

Enterprise Features:
- Configuration externalization with EnterpriseConfigurationManager
- Advanced monitoring integration with metrics collection
- Enterprise error handling and recovery strategies
- Memory optimization with resource planning
- Professional PySpark code with type hints
"""
import logging
from typing import Dict, Any, Optional, List
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *
import yaml
from datetime import datetime

# Enterprise imports
from .base_classes import BaseMapping, BaseTransformation
from .config_management import EnterpriseConfigurationManager
from .monitoring_integration import MonitoringIntegration
from .advanced_config_validation import ConfigValidator

class {{ mapping.name | replace(' ', '') | replace('-', '_') }}(BaseMapping):
    """
    Enterprise mapping class for {{ mapping.name }}
    
    Generated Components: {{ transformations | length }} transformations
    {% if dag_analysis %}
    Execution Plan: {{ dag_analysis.execution_plan.total_phases }} phases
    Estimated Duration: {{ dag_analysis.estimated_duration_seconds }} seconds
    {% endif %}
    """
    
    def __init__(self, spark_session: SparkSession, config: Dict[str, Any]):
        super().__init__(spark_session, config, "{{ mapping.name }}")
        
        # Enterprise configuration management
        self.config_manager = EnterpriseConfigurationManager(config)
        self.monitoring = MonitoringIntegration(self.config_manager)
        self.config_validator = ConfigValidator()
        
        # Validate configuration
        validation_errors = self.config_validator.validate_mapping_config(config)
        if validation_errors:
            raise ValueError(f"Configuration validation failed: {validation_errors}")
            
        # Initialize connections
        self.connections = self._initialize_connections()
        
        # Performance monitoring
        self.performance_metrics = {
            'start_time': None,
            'end_time': None,
            'rows_processed': 0,
            'transformation_times': {},
            'memory_usage': {}
        }
        
        self.logger.info(f"‚úÖ Initialized {{ mapping.name }} with enterprise features")
        
    def execute(self) -> bool:
        """
        Execute the mapping with enterprise monitoring and error handling
        
        Returns:
            True if execution successful, False otherwise
        """
        self.performance_metrics['start_time'] = datetime.now()
        
        try:
            self.logger.info(f"üöÄ Starting execution of {{ mapping.name }}")
            self.monitoring.start_mapping_execution(self.mapping_name)
            
            {% if dag_analysis and dag_analysis.execution_plan %}
            # Execute in optimized phases
            {% for phase in dag_analysis.execution_plan.phases %}
            self._execute_phase_{{ phase.phase_number }}()
            {% endfor %}
            {% else %}
            # Execute transformations
            self._execute_transformations()
            {% endif %}
            
            self.performance_metrics['end_time'] = datetime.now()
            duration = (self.performance_metrics['end_time'] - self.performance_metrics['start_time']).total_seconds()
            
            self.logger.info(f"‚úÖ {{ mapping.name }} completed successfully in {duration:.2f}s")
            self.monitoring.complete_mapping_execution(self.mapping_name, True, duration)
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå {{ mapping.name }} failed: {str(e)}")
            self.monitoring.complete_mapping_execution(self.mapping_name, False, 0)
            
            # Enterprise error handling
            if self.config_manager.get_setting('error_handling.fail_on_error', True):
                raise
            else:
                return False
                
    {% if dag_analysis and dag_analysis.execution_plan %}
    {% for phase in dag_analysis.execution_plan.phases %}
    def _execute_phase_{{ phase.phase_number }}(self):
        """Execute phase {{ phase.phase_number }}: {{ phase.transformations | join(', ') }}"""
        phase_start = datetime.now()
        
        try:
            self.logger.info(f"üìä Executing phase {{ phase.phase_number }} with {{ phase.transformations | length }} transformations")
            
            {% if phase.can_parallelize %}
            # Parallel execution for independent transformations
            {% for transformation in phase.transformations %}
            self._execute_{{ transformation | lower | replace(' ', '_') }}()
            {% endfor %}
            {% else %}
            # Sequential execution
            {% for transformation in phase.transformations %}
            self._execute_{{ transformation | lower | replace(' ', '_') }}()
            {% endfor %}
            {% endif %}
            
            phase_duration = (datetime.now() - phase_start).total_seconds()
            self.performance_metrics['transformation_times']['phase_{{ phase.phase_number }}'] = phase_duration
            self.logger.debug(f"‚úÖ Phase {{ phase.phase_number }} completed in {phase_duration:.2f}s")
            
        except Exception as e:
            self.logger.error(f"‚ùå Phase {{ phase.phase_number }} failed: {str(e)}")
            raise
    {% endfor %}
    {% endif %}
    
    def _execute_transformations(self):
        """Execute transformations with enterprise monitoring"""
        {% for transformation in transformations %}
        self._execute_{{ transformation.name | lower | replace(' ', '_') }}()
        {% endfor %}
        
    {% for transformation in transformations %}
    def _execute_{{ transformation.name | lower | replace(' ', '_') }}(self):
        """Execute {{ transformation.name }} ({{ transformation.type }})"""
        trans_start = datetime.now()
        
        try:
            self.logger.debug(f"üîß Executing {{ transformation.name }}")
            
            # TODO: Implement {{ transformation.type }} transformation logic
            # This would be generated based on the specific transformation configuration
            
            trans_duration = (datetime.now() - trans_start).total_seconds()
            self.performance_metrics['transformation_times']['{{ transformation.name }}'] = trans_duration
            
            self.monitoring.record_transformation_metric(
                '{{ transformation.name }}', 
                trans_duration, 
                self.performance_metrics.get('rows_processed', 0)
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå {{ transformation.name }} failed: {str(e)}")
            raise
    {% endfor %}
    
    def _initialize_connections(self) -> Dict[str, Any]:
        """Initialize enterprise connections with pooling and retry logic"""
        connections = {}
        
        {% for connection in connections %}
        # Initialize {{ connection.name }}
        connections['{{ connection.name }}'] = self.config_manager.get_connection('{{ connection.name }}')
        {% endfor %}
        
        return connections
        
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics"""
        return {
            **self.performance_metrics,
            'mapping_name': self.mapping_name,
            'total_transformations': {{ transformations | length }},
            {% if dag_analysis %}
            'optimization_applied': True,
            'execution_phases': {{ dag_analysis.execution_plan.total_phases }},
            'parallelizable_phases': {{ dag_analysis.execution_plan.parallelizable_phases }}
            {% else %}
            'optimization_applied': False
            {% endif %}
        }
        
    def cleanup(self):
        """Enterprise cleanup with resource management"""
        try:
            # Close connections
            for conn_name, connection in self.connections.items():
                if hasattr(connection, 'close'):
                    connection.close()
                    
            # Clear caches
            self.spark.catalog.clearCache()
            
            # Log final metrics
            metrics = self.get_performance_metrics()
            self.monitoring.log_final_metrics(metrics)
            
            self.logger.info("üßπ Enterprise cleanup completed")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Cleanup warning: {str(e)}")
'''
```

## ‚úÖ Testing the Code Generator

Create comprehensive tests in `tests/test_spark_code_generator.py`:

```python
"""
Tests for Spark code generator
"""
import pytest
import tempfile
import shutil
from pathlib import Path

from src.core.spark_code_generator import SparkCodeGenerator
from src.core.base_classes import Project, Folder, Mapping, Transformation

class TestSparkCodeGenerator:
    """Test Spark code generator functionality"""
    
    def setup_method(self):
        """Set up test environment"""
        self.temp_dir = Path(tempfile.mkdtemp())
        self.generator = SparkCodeGenerator(str(self.temp_dir))
        
    def teardown_method(self):
        """Clean up test environment"""
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
            
    def test_generator_initialization(self):
        """Test generator initialization"""
        assert self.generator.output_base_dir == self.temp_dir
        assert self.generator.enable_config_externalization == True
        assert self.generator.enterprise_features == True
        
    def test_application_structure_creation(self):
        """Test application structure creation"""
        app_dir = self.temp_dir / "TestApp"
        self.generator._create_application_structure(app_dir)
        
        # Check that directories were created
        assert (app_dir / "src" / "main" / "python").exists()
        assert (app_dir / "config").exists()
        assert (app_dir / "data" / "input").exists()
        assert (app_dir / "logs").exists()
        
        # Check that __init__.py files were created
        assert (app_dir / "src" / "main" / "python" / "__init__.py").exists()
        
    def test_template_selection(self):
        """Test mapping template selection"""
        # Create mock mapping with different complexities
        simple_mapping = type('Mapping', (), {'name': 'SimpleMapping', 'transformations': [1, 2]})
        complex_mapping = type('Mapping', (), {'name': 'ComplexMapping', 'transformations': list(range(15))})
        
        simple_template = self.generator._select_mapping_template(simple_mapping, {})
        complex_template = self.generator._select_mapping_template(complex_mapping, {})
        
        assert "lean" in simple_template.lower()
        assert "enterprise" in complex_template.lower()
        
    def test_requirements_generation(self):
        """Test requirements.txt generation"""
        requirements = self.generator._get_requirements_content()
        
        assert "pyspark" in requirements.lower()
        assert "pyyaml" in requirements.lower()
        assert "pandas" in requirements.lower()
        
    def test_run_script_generation(self):
        """Test run.sh script generation"""
        script_content = self.generator._get_run_script_content("TestApp")
        
        assert "spark-submit" in script_content
        assert "TestApp" in script_content
        assert "#!/bin/bash" in script_content
```

## ‚úÖ Verification Steps

After implementing the Spark code generator:

```bash
# 1. Test code generator
pytest tests/test_spark_code_generator.py -v

# 2. Generate a sample application
python -c "
from src.core.spark_code_generator import SparkCodeGenerator
gen = SparkCodeGenerator()
# This would need a real XML file
print('‚úÖ Generator initialized')
"

# 3. Test template loading
python -c "
from src.core.spark_code_generator import SparkCodeGenerator
gen = SparkCodeGenerator()
template = gen.jinja_env.get_template('enterprise_ultra_lean_template.py') if gen.jinja_env.list_templates() else None
print('‚úÖ Template system works')
"

# 4. Test DAG processor
python -c "
from src.core.mapping_dag_processor import MappingDAGProcessor
processor = MappingDAGProcessor()
print('‚úÖ DAG processor initialized')
"
```

## üîó Next Steps

With the Spark code generator implemented, proceed to:
1. **`09_WORKFLOW_ORCHESTRATION.md`** - Implement workflow DAG processing
2. **`10_TEMPLATES_AND_FORMATTING.md`** - Complete the template system
3. **`15_COMPREHENSIVE_TESTING.md`** - Add comprehensive testing

The Spark code generator now provides:
- ‚úÖ Professional PySpark application generation
- ‚úÖ Enterprise-grade code templates
- ‚úÖ DAG optimization and parallel execution planning
- ‚úÖ Configuration externalization
- ‚úÖ Complete deployment file generation
- ‚úÖ Professional code formatting with Black
- ‚úÖ Comprehensive monitoring integration
- ‚úÖ Docker containerization support